{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 275,
          "status": "ok",
          "timestamp": 1710450166603,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 0
        },
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/moficodes/ai-on-gke/blob/main/tutorials/finetune-gemma-2b-on-l4/finetune-gemma-on-gke.ipynb\" target=\"_blank\"><img height=\"40\" alt=\"Run your own notebook in Colab\" src = \"https://colab.research.google.com/assets/colab-badge.svg\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Finetune Gemma to GKE using GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates downloading and fine tuning Gemma, open models from Google DeepMind using Pytorch and Hugging Face Libraries In this notebook we will finetune and publish Gemma model on Hugging Face. In this guide we specifically use L4 GPUs but this guide should also work for A100(40 GB), A100(80 GB), H100(80 GB) GPUs.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "Finetune and Publish Gemma with Transformers and Lora on GPUs.\n",
        "\n",
        "### GPUs\n",
        "\n",
        "GPUs let you accelerate specific workloads running on your nodes such as machine learning and data processing. GKE provides a range of machine type options for node configuration, including machine types with NVIDIA H100, L4, and A100 GPUs.\n",
        "\n",
        "Before you use GPUs in GKE, we recommend that you complete the following learning path:\n",
        "\n",
        "Learn about [current GPU version availability](https://cloud.google.com/compute/docs/gpus)\n",
        "\n",
        "Learn about [GPUs in GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/gpus)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35Dvbzb0hH3-"
      },
      "source": [
        "### Configure Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c460088b873"
      },
      "source": [
        "Set the following variables for the experiment environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 276,
          "status": "ok",
          "timestamp": 1710459545977,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 0
        },
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# The HuggingFace token used to download models.\n",
        "# Make sure Token has Write Permission\n",
        "HF_TOKEN = \"<YOUR_HF_TOKEN>\"  # @param {type:\"string\"}\n",
        "\n",
        "# The size of the model to launch\n",
        "MODEL_SIZE = \"2b\"  # @param [\"2b\", \"7b\"]\n",
        "\n",
        "# Cloud project id.\n",
        "PROJECT_ID = \"<YOUR_PROJECT_ID>\"  # @param {type:\"string\"}\n",
        "\n",
        "# Region for launching clusters.\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# The cluster name to create\n",
        "CLUSTER_NAME = \"gke-gemma-cluster\"  # @param {type:\"string\"}\n",
        "\n",
        "# The number of GPUs to run\n",
        "GPU_COUNT = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klPAnx16cVd7"
      },
      "outputs": [],
      "source": [
        "! gcloud auth login\n",
        "! gcloud config set project \"$PROJECT_ID\"\n",
        "! gcloud services enable container.googleapis.com\n",
        "\n",
        "# Add kubectl to the set of available tools.\n",
        "! mkdir -p /tools/google-cloud-sdk/.install\n",
        "! gcloud components install kubectl --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e828eb320337"
      },
      "source": [
        "### Create a GKE cluster and a node pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKhdKv1vK9Lg"
      },
      "source": [
        "GKE creates the following resources for the model based on the MODEL_SIZE environment variable set above.\n",
        "\n",
        "- Autopilot cluster\n",
        "\n",
        "If you already have a cluster, you can skip to `Use an existing GKE cluster` instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12cd25839741"
      },
      "outputs": [],
      "source": [
        "! gcloud container clusters create-auto {CLUSTER_NAME} \\\n",
        "  --project={PROJECT_ID} \\\n",
        "  --region={REGION} \\\n",
        "  --release-channel=rapid \\\n",
        "  --cluster-version=1.29"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ydvYk7FLJz_"
      },
      "source": [
        "### Use an existing GKE cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1488,
          "status": "ok",
          "timestamp": 1710451982779,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 0
        },
        "id": "DmpNpYF-LRut",
        "outputId": "84dbdcb1-2daa-4be8-cb26-18bab4848d85"
      },
      "outputs": [],
      "source": [
        "! gcloud container clusters get-credentials {CLUSTER_NAME} --location {REGION}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cc825514deb"
      },
      "source": [
        "### Create Kubernetes secret for Hugging Face credentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgZfNOSyOY7_"
      },
      "source": [
        "Create a Kubernetes Secret that contains the Hugging Face token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1057,
          "status": "ok",
          "timestamp": 1710459552596,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 0
        },
        "id": "b42bd4fa2b2d",
        "outputId": "7b55174a-02db-41c6-ff77-00dba16d20df"
      },
      "outputs": [],
      "source": [
        "! kubectl create secret generic hf-secret \\\n",
        "--from-literal=hf_api_token={HF_TOKEN} \\\n",
        "--dry-run=client -o yaml | kubectl apply -f -"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOADJkZAmt08"
      },
      "source": [
        "## The Dataset\n",
        "We use Lora to quickly finetune Gemma with `b-mc2/sql-create-context` dataset.\n",
        "\n",
        "This dataset has the following structure.\n",
        "\n",
        "| Answer                                              | Question                                                                      | Context                                                           |\n",
        "|-----------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------|\n",
        "| SELECT COUNT(*) FROM head WHERE age > 56            | How many heads of the departments are older than 56 ?                         | CREATE TABLE head (age INTEGER)                                   |\n",
        "| SELECT name, born_state, age FROM head ORDER BY age | List the name, born state and age of the heads of departments ordered by age. | CREATE TABLE head (name VARCHAR, born_state VARCHAR, age VARCHAR) |\n",
        "\n",
        "We will finetune `google/gemma-2b` model to get SQL queries based on questions and context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JrfaiA5tAtW"
      },
      "source": [
        "## Finetuning Gemma on GKE using GPU with Pytorch\n",
        "\n",
        "In this demo we will use Pytorch and Huggingface libraries to finetune Gemma. We use the `finetune.py` file.\n",
        "\n",
        "```python\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = os.getenv(\"MODEL_NAME\", \"google/gemma-2b\")\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"b-mc2/sql-create-context\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = os.getenv(\"NEW_MODEL\", \"gemma-2b-sql\")\n",
        "\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = int(os.getenv(\"LORA_R\", \"4\"))\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = int(os.getenv(\"LORA_ALPHA\", \"8\"))\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = True\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = int(os.getenv(\"TRAIN_BATCH_SIZE\", \"1\"))\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = int(os.getenv(\"EVAL_BATCH_SIZE\", \"2\"))\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = int(os.getenv(\"GRADIENT_ACCUMULATION_STEPS\", \"1\"))\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 0\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = int(os.getenv(\"LOGGING_STEPS\", \"50\"))\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = int(os.getenv(\"MAX_SEQ_LENGTH\", \"512\"))\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {'':torch.cuda.current_device()}\n",
        "\n",
        "# Set limit to a positive number\n",
        "limit = int(os.getenv(\"DATASET_LIMIT\", \"5000\"))\n",
        "\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "if limit != -1:\n",
        "    dataset = dataset.shuffle(seed=42).select(range(limit))\n",
        "\n",
        "\n",
        "def transform(data):\n",
        "    question = data['question']\n",
        "    context = data['context']\n",
        "    answer = data['answer']\n",
        "    template = \"Question: {question}\\nContext: {context}\\nAnswer: {answer}\"\n",
        "    return {'text': template.format(question=question, context=context, answer=answer)}\n",
        "\n",
        "\n",
        "transformed = dataset.map(transform)\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load base model\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=transformed,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.model.save_pretrained(new_model)\n",
        "\n",
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "\n",
        "\n",
        "model.push_to_hub(new_model, check_pr=True)\n",
        "\n",
        "tokenizer.push_to_hub(new_model, check_pr=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XceN-WKxRR-"
      },
      "source": [
        "## Create a Container Manifest with Dockerfile\n",
        "\n",
        "Use the following `Dockerfile` to create a container image.\n",
        "\n",
        "```bash\n",
        "FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04\n",
        "\n",
        "RUN apt-get update && \\\n",
        "    apt-get -y --no-install-recommends install python3-dev gcc python3-pip git && \\\n",
        "    rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "RUN pip3 install --no-cache-dir accelerate bitsandbytes datasets transformers peft trl torch\n",
        "\n",
        "COPY finetune.py /finetune.py\n",
        "\n",
        "ENV PYTHONUNBUFFERED 1\n",
        "\n",
        "CMD python3 /finetune.py --device cuda\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 273,
          "status": "ok",
          "timestamp": 1710457873562,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 0
        },
        "id": "pCwBFXuTxaeU"
      },
      "outputs": [],
      "source": [
        "DOCKERFILE = \"\"\"\n",
        "FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04\n",
        "\n",
        "RUN apt-get update && \\\n",
        "    apt-get -y --no-install-recommends install python3-dev gcc python3-pip git && \\\n",
        "    rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "RUN pip3 install --no-cache-dir accelerate bitsandbytes datasets transformers peft trl torch\n",
        "\n",
        "COPY finetune.py /finetune.py\n",
        "\n",
        "ENV PYTHONUNBUFFERED 1\n",
        "\n",
        "CMD python3 /finetune.py --device cuda\n",
        "\"\"\"\n",
        "\n",
        "with open(\"Dockerfile\", \"w\") as f:\n",
        "    f.write(DOCKERFILE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdqyOdL4tlOP"
      },
      "source": [
        "### Containerize the Code with Docker and Cloud Build\n",
        "\n",
        "Using Cloud Build and the following Dockerfile we build and push the image in Artifact Registry Docker Repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKk9B_8cwNrL"
      },
      "outputs": [],
      "source": [
        "# Create a Artifact Registry Repo\n",
        "! gcloud artifacts repositories create gemma \\\n",
        "    --project={PROJECT_ID} \\\n",
        "    --repository-format=docker \\\n",
        "    --location=us \\\n",
        "    --description=\"Gemma Repo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 636524,
          "status": "ok",
          "timestamp": 1710455573555,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 0
        },
        "id": "mYo1j99-zXWe",
        "outputId": "e6fefc69-27d5-4b98-fa4e-cc5584be97d4"
      },
      "outputs": [],
      "source": [
        "# Build and push the image using Cloud Build\n",
        "! gcloud builds submit \\\n",
        "    --tag us-docker.pkg.dev/{PROJECT_ID}/gemma/finetune-gemma-gpu:1.0.0 ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c250872074f"
      },
      "source": [
        "## Run Finetune Job on GKE Autopilot\n",
        "\n",
        "Use the YAML to run Gemma Finetune on GKE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 255,
          "status": "ok",
          "timestamp": 1710457951607,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 0
        },
        "id": "6psJZY_zUDgj"
      },
      "outputs": [],
      "source": [
        "K8S_JOB_YAML = f\"\"\"\n",
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "apiVersion: batch/v1\n",
        "kind: Job\n",
        "metadata:\n",
        "  name: finetune-job\n",
        "  namespace: default\n",
        "spec:\n",
        "  backoffLimit: 2\n",
        "  template:\n",
        "    metadata:\n",
        "      annotations:\n",
        "        kubectl.kubernetes.io/default-container: finetuner\n",
        "    spec:\n",
        "      terminationGracePeriodSeconds: 600\n",
        "      containers:\n",
        "      - name: finetuner\n",
        "        image: <YOUR_IMAGE>\n",
        "        resources:\n",
        "          limits:\n",
        "            nvidia.com/gpu: 8\n",
        "        env:\n",
        "        - name: MODEL_NAME\n",
        "          value: \"google/gemma-2b\"\n",
        "        - name: NEW_MODEL\n",
        "          value: \"gemma-2b-sql-kubecon-eu-2024\"\n",
        "        - name: LORA_R\n",
        "          value: \"8\"\n",
        "        - name: LORA_ALPHA\n",
        "          value: \"16\"\n",
        "        - name: TRAIN_BATCH_SIZE\n",
        "          value: \"1\"\n",
        "        - name: EVAL_BATCH_SIZE\n",
        "          value: \"2\"\n",
        "        - name: GRADIENT_ACCUMULATION_STEPS\n",
        "          value: \"2\"\n",
        "        - name: DATASET_LIMIT\n",
        "          value: \"1000\"\n",
        "        - name: MAX_SEQ_LENGTH\n",
        "          value: \"512\"\n",
        "        - name: LOGGING_STEPS\n",
        "          value: \"5\"\n",
        "        - name: HF_TOKEN\n",
        "          valueFrom:\n",
        "            secretKeyRef:\n",
        "              name: hf-secret\n",
        "              key: hf_api_token\n",
        "        volumeMounts:\n",
        "        - mountPath: /dev/shm\n",
        "          name: dshm\n",
        "      volumes:\n",
        "      - name: dshm\n",
        "        emptyDir:\n",
        "          medium: Memory\n",
        "      nodeSelector:\n",
        "        cloud.google.com/gke-accelerator: nvidia-l4\n",
        "      restartPolicy: OnFailure\n",
        "\"\"\"\n",
        "\n",
        "with open(\"finetune.yaml\", \"w\") as f:\n",
        "    f.write(K8S_JOB_YAML)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 536,
          "status": "ok",
          "timestamp": 1710459879982,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 0
        },
        "id": "yTmQqcRj9kz_",
        "outputId": "5da8309a-56f0-493b-ce6e-1b786bfdf97f"
      },
      "outputs": [],
      "source": [
        "!kubectl apply -f finetune.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYMesXi7WqCu"
      },
      "source": [
        "#### Waiting for the container to create"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKwbzKXuWvoL"
      },
      "source": [
        "Use the command below to check on the status of the container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 288136,
          "status": "ok",
          "timestamp": 1710460170411,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 0
        },
        "id": "PXbPCrWtWqbk",
        "outputId": "855c8166-1436-4529-e061-39c6d55d8eea"
      },
      "outputs": [],
      "source": [
        "! kubectl get po -l job-name=finetune-job -w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzINwFr_WVAB"
      },
      "source": [
        "### View the logs from the running Job\n",
        "\n",
        "This will download the needed artifacts and run the finetuning code, this process will take close to 30 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 32451,
          "status": "ok",
          "timestamp": 1710460809272,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 0
        },
        "id": "gAkXSoy9Ufuo",
        "outputId": "c41f3666-92a3-4627-96a9-9f4e15bc33a7"
      },
      "outputs": [],
      "source": [
        "! kubectl logs -f job/finetune-job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VosC4Fbb01S9"
      },
      "source": [
        "## Find the model on Huggingface\n",
        "\n",
        "If the Job ran successfully you can now go find the model on your Huggingface profile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af21a3cff1e0"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "! kubectl delete job finetune-job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mH9DlYk3IqA5"
      },
      "outputs": [],
      "source": [
        "! kubectl delete secrets hf-secret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2acSuqPeNjJJ"
      },
      "outputs": [],
      "source": [
        "! gcloud container clusters delete {CLUSTER_NAME} \\\n",
        "  --region={REGION} \\\n",
        "  --quiet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_gemma_deployment_on_gke.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
