{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/moficodes/ai-on-gke/blob/main/tutorials/finetune-gemma-7b-on-tpu/finetune-gemma-on-gke-using-TPU.ipynb\" target=\"_blank\"><img height=\"40\" alt=\"Run your own notebook in Colab\" src = \"https://colab.research.google.com/assets/colab-badge.svg\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Finetune Gemma on GKE using TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates downloading and fine tuning Gemma, open models from Google DeepMind using Pytorch and Hugging Face Libraries. In this notebook we will finetune and publish Gemma model on Hugging Face. In this guide we specifically use TPU V4 but this guide should also work for any TPU version with enough memory.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "Finetune and Publish Gemma with Transformers and Lora on TPUs.\n",
        "\n",
        "### TPUs\n",
        "\n",
        "Tensor Processing Units (TPUs) are Google's custom-developed application-specific integrated circuits (ASICs) used to accelerate machine learning workloads.\n",
        "\n",
        "Before you use TPUs in GKE, we recommend that you complete the following learning path:\n",
        "\n",
        "Learn about [TPUs in GKE](https://cloud.google.com/tpu/docs/tpus-in-gke)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35Dvbzb0hH3-"
      },
      "source": [
        "### Configure Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c460088b873"
      },
      "source": [
        "Set the following variables for the experiment environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# The HuggingFace token used to download models.\n",
        "# Make sure Token has Write Permission\n",
        "HF_TOKEN = \"<YOUR_HF_TOKEN>\"  # @param {type:\"string\"}\n",
        "\n",
        "# The size of the model to launch\n",
        "MODEL_SIZE = \"7b\"  # @param [\"2b\", \"7b\"]\n",
        "\n",
        "# Cloud project id.\n",
        "PROJECT_ID = \"<YOUR_PROJECT_ID>\"  # @param {type:\"string\"}\n",
        "\n",
        "# Region for launching clusters.\n",
        "REGION = \"us-central2\"  # @param {type:\"string\"}\n",
        "\n",
        "LOCATION = \"us-central2-a\"  # @param {type:\"string\"}\n",
        "\n",
        "# The cluster name to create\n",
        "CLUSTER_NAME = \"keras\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 136218,
          "status": "ok",
          "timestamp": 1710694911968,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "klPAnx16cVd7",
        "outputId": "55548855-83ec-4f14-aabc-de0d5cf40b4d"
      },
      "outputs": [],
      "source": [
        "! gcloud auth login\n",
        "! gcloud config set project \"$PROJECT_ID\"\n",
        "! gcloud services enable container.googleapis.com\n",
        "# If using in public colab need to login with gcloud auth login\n",
        "\n",
        "# Add kubectl to the set of available tools.\n",
        "! mkdir -p /tools/google-cloud-sdk/.install\n",
        "! gcloud components install kubectl --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e828eb320337"
      },
      "source": [
        "### Create a GKE cluster and a node pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKhdKv1vK9Lg"
      },
      "source": [
        "GKE creates the following resources for the model based on the MODEL_SIZE environment variable set above.\n",
        "\n",
        "- Standard cluster\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "If you already have a cluster, you can skip to `Use an existing GKE cluster` instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12cd25839741"
      },
      "outputs": [],
      "source": [
        "! gcloud container --project {CLUSTER_NAME} clusters create {CLUSTER_NAME} \\\n",
        "    --cluster-version \"1.29.2-gke.1217000\" \\\n",
        "    --release-channel \"rapid\" \\\n",
        "    --machine-type \"n1-standard-4\" \\\n",
        "    --num-nodes \"1\" \\\n",
        "    --node-locations {LOCATION}\n",
        "\n",
        "! gcloud container --project {CLUSTER_NAME} node-pools create \"tpu\" \\\n",
        "    --cluster {CLUSTER_NAME} \\\n",
        "    --node-version \"1.29.2-gke.1217000\" \\\n",
        "    --machine-type \"ct4p-hightpu-4t\"  \\\n",
        "    --num-nodes \"4\" \\\n",
        "    --placement-type=COMPACT \\\n",
        "    --tpu-topology=2x2x4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ydvYk7FLJz_"
      },
      "source": [
        "### Use an existing GKE cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1572,
          "status": "ok",
          "timestamp": 1710695045224,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "DmpNpYF-LRut",
        "outputId": "c8aa9c79-fc87-479a-814a-b79704c99684"
      },
      "outputs": [],
      "source": [
        "! gcloud container clusters \\\n",
        "    get-credentials {CLUSTER_NAME} \\\n",
        "    --location {LOCATION}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cc825514deb"
      },
      "source": [
        "### Create Kubernetes secret for Hugging Face credentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgZfNOSyOY7_"
      },
      "source": [
        "Create a Kubernetes Secret that contains the Hugging Face token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4478,
          "status": "ok",
          "timestamp": 1710695064954,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "b42bd4fa2b2d",
        "outputId": "63b871d5-606b-4ee1-9cae-5a254c62f61f"
      },
      "outputs": [],
      "source": [
        "! kubectl create secret generic hf-secret \\\n",
        "--from-literal=hf_api_token={HF_TOKEN} \\\n",
        "--dry-run=client -o yaml | kubectl apply -f -"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOADJkZAmt08"
      },
      "source": [
        "## The Dataset\n",
        "We use Lora to quickly finetune Gemma with `b-mc2/sql-create-context` dataset.\n",
        "\n",
        "This dataset has the following structure.\n",
        "\n",
        "| Answer                                              | Question                                                                      | Context                                                           |\n",
        "|-----------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------|\n",
        "| SELECT COUNT(*) FROM head WHERE age > 56            | How many heads of the departments are older than 56 ?                         | CREATE TABLE head (age INTEGER)                                   |\n",
        "| SELECT name, born_state, age FROM head ORDER BY age | List the name, born state and age of the heads of departments ordered by age. | CREATE TABLE head (name VARCHAR, born_state VARCHAR, age VARCHAR) |\n",
        "\n",
        "We will finetune `google/gemma-7b` model to get SQL queries based on questions and context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JrfaiA5tAtW"
      },
      "source": [
        "## Finetuning Gemma on GKE using GPU with Pytorch\n",
        "\n",
        "In this demo we will use Pytorch-XLA and Huggingface libraries to finetune Gemma. Save the following code in a file named `fsdp.py`\n",
        "\n",
        "```python\n",
        "# Make sure to run the script with the following envs:\n",
        "#   PJRT_DEVICE=TPU XLA_USE_SPMD=1\n",
        "import os\n",
        "import torch\n",
        "import torch_xla\n",
        "\n",
        "import torch_xla.core.xla_model as xm\n",
        "\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "import transformers\n",
        "\n",
        "print(\"TORCH: \", torch.__version__)\n",
        "print(\"TRANSFORMERS: \", transformers.__version__)\n",
        "\n",
        "# Set up TPU device.\n",
        "device = xm.xla_device()\n",
        "model_id = os.getenv(\"MODEL_ID\",\"google/gemma-7b\")\n",
        "new_model_id = os.getenv(\"NEW_MODEL_ID\",\"gemma-7b-sql-context\")\n",
        "\n",
        "job_index = os.getenv(\"JOB_COMPLETION_INDEX\")\n",
        "\n",
        "print(\"### LOAD TOKENIZER ###\")\n",
        "# Load the pretrained model and tokenizer.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "\n",
        "print(\"### LOAD MODEL ###\")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Set up PEFT LoRA for fine-tuning.\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.1,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "print(\"### LOAD DATASET ###\")\n",
        "\n",
        "limit = int(os.getenv(\"LIMIT\", \"5000\"))\n",
        "\n",
        "dataset_name = \"b-mc2/sql-create-context\"\n",
        "# Load the dataset and format it for training.\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "dataset = dataset.shuffle(seed=42).select(range(limit))\n",
        "\n",
        "def transform(data):\n",
        "    question = data['question']\n",
        "    context = data['context']\n",
        "    answer = data['answer']\n",
        "    template = \"Question: {question}\\nContext: {context}\\nAnswer: {answer}\"\n",
        "    return {'text': template.format(question=question, context=context, answer=answer)}\n",
        "\n",
        "print(\"### TRANSFORM DATASET ###\")\n",
        "dataset = dataset.map(transform)\n",
        "\n",
        "\n",
        "max_seq_length = 512\n",
        "\n",
        "# Set up the FSDP config. To enable FSDP via SPMD, set xla_fsdp_v2 to True.\n",
        "fsdp_config = {\"fsdp_transformer_layer_cls_to_wrap\": [\n",
        "        \"GemmaDecoderLayer\"\n",
        "    ],\n",
        "    \"xla\": True,\n",
        "    \"xla_fsdp_v2\": True,\n",
        "    \"xla_fsdp_grad_ckpt\": True}\n",
        "\n",
        "print(\"### CREATE SFTTRAINER###\")\n",
        "# Finally, set up the trainer and train the model.\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=64,  # This is actually the global batch size for SPMD.\n",
        "        num_train_epochs=1,\n",
        "        max_steps=-1,\n",
        "        output_dir=\"./output\",\n",
        "        optim=\"adafactor\",\n",
        "        logging_steps=1,\n",
        "        dataloader_drop_last = True,  # Required for SPMD.\n",
        "        fsdp=\"full_shard\",\n",
        "        fsdp_config=fsdp_config,\n",
        "    ),\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    packing=True,\n",
        ")\n",
        "\n",
        "\n",
        "print(\"### STARTING TRAINING ###\")\n",
        "trainer.train()\n",
        "print(\"### TRAINING ENDED ###\")\n",
        "\n",
        "\n",
        "print(\"JOB INDEX: \", job_index)\n",
        "\n",
        "print(\"### COMBINE AND MODEL WEIGHT ###\")\n",
        "trainer.save_model(new_model_id)\n",
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, new_model_id)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "print(\"### DONE MERGING ###\")\n",
        "\n",
        "if job_index == \"0\":\n",
        "    print(\"### UPLOAD MODEL TO HUGGING FACE ###\")\n",
        "    # model.config.to_json_file(\"adapter_config.json\")\n",
        "    print(model)\n",
        "    os.listdir(new_model_id)\n",
        "    model.push_to_hub(repo_id=new_model_id)\n",
        "    tokenizer.push_to_hub(repo_id=new_model_id)\n",
        "else:\n",
        "    print(\"Model will be uploaded by job 0\")\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XceN-WKxRR-"
      },
      "source": [
        "## Create a Container Manifest with Dockerfile\n",
        "\n",
        "Use the following `Dockerfile` to create a container image.\n",
        "\n",
        "```bash\n",
        "FROM us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:nightly_3.10_tpuvm_20240229\n",
        "\n",
        "RUN pip install -U git+https://github.com/huggingface/transformers.git\n",
        "RUN pip install -U git+https://github.com/huggingface/trl.git\n",
        "RUN pip install -U datasets peft\n",
        "\n",
        "COPY . .\n",
        "\n",
        "CMD python fsdp.py\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 482,
          "status": "ok",
          "timestamp": 1710701636584,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "pCwBFXuTxaeU"
      },
      "outputs": [],
      "source": [
        "DOCKERFILE = \"\"\"\n",
        "FROM us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:nightly_3.10_tpuvm_20240229\n",
        "\n",
        "RUN pip install -U git+https://github.com/huggingface/transformers.git\n",
        "RUN pip install -U git+https://github.com/huggingface/trl.git\n",
        "RUN pip install -U datasets peft\n",
        "\n",
        "COPY . .\n",
        "\n",
        "CMD python fsdp.py\n",
        "\"\"\"\n",
        "\n",
        "with open(\"Dockerfile\", \"w\") as f:\n",
        "    f.write(DOCKERFILE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdqyOdL4tlOP"
      },
      "source": [
        "### Containerize the Code with Docker and Cloud Build\n",
        "\n",
        "Using Cloud Build and the following Dockerfile we build and push the image in Artifact Registry Docker Repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKk9B_8cwNrL"
      },
      "outputs": [],
      "source": [
        "# Create a Artifact Registry Repo\n",
        "! gcloud artifacts repositories create gemma \\\n",
        "    --project={PROJECT_ID} \\\n",
        "    --repository-format=docker \\\n",
        "    --location=us \\\n",
        "    --description=\"Gemma Repo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 172237,
          "status": "ok",
          "timestamp": 1710698643369,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "mYo1j99-zXWe",
        "outputId": "2f71febf-d982-494f-d5bb-e8262c827798"
      },
      "outputs": [],
      "source": [
        "# Build and push the image using Cloud Build\n",
        "! gcloud builds submit \\\n",
        "    --tag us-docker.pkg.dev/{PROJECT_ID}/gemma/finetune-gemma-tpu:1.0.1 ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c250872074f"
      },
      "source": [
        "## Run Finetune Job on GKE\n",
        "\n",
        "Use the YAML to run Gemma Finetune on GKE. Notice we have a job with a headless service. This is because we have a TPU V4 2x2x4 slice which is 4 Nodes with 4 TPU devices each connected via high speed interconnect. We create a indexed job and headless service to give each instance of the job to have be able to communicate with each other via network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6psJZY_zUDgj"
      },
      "outputs": [],
      "source": [
        "K8S_JOB_YAML = f\"\"\"\n",
        "apiVersion: v1\n",
        "kind: Service\n",
        "metadata:\n",
        "  name: headless-svc\n",
        "spec:\n",
        "  clusterIP: None\n",
        "  selector:\n",
        "    job-name: tpu-job\n",
        "---\n",
        "apiVersion: batch/v1\n",
        "kind: Job\n",
        "metadata:\n",
        "  name: tpu-job\n",
        "spec:\n",
        "  backoffLimit: 0\n",
        "  completions: 4\n",
        "  parallelism: 4\n",
        "  completionMode: Indexed\n",
        "  template:\n",
        "    spec:\n",
        "      subdomain: headless-svc\n",
        "      restartPolicy: Never\n",
        "      nodeSelector:\n",
        "        cloud.google.com/gke-tpu-accelerator: tpu-v4-podslice\n",
        "        cloud.google.com/gke-tpu-topology: 2x2x4\n",
        "      containers:\n",
        "      - name: tpu-job\n",
        "        image: us-docker.pkg.dev/{PROJECT_ID}/gemma/finetune-gemma-tpu:1.0.1\n",
        "        ports:\n",
        "        - containerPort: 8471 # Default port using which TPU VMs communicate\n",
        "        - containerPort: 8431 # Port to export TPU runtime metrics, if supported.\n",
        "        securityContext:\n",
        "          privileged: true\n",
        "        resources:\n",
        "          requests:\n",
        "            google.com/tpu: 4\n",
        "          limits:\n",
        "            google.com/tpu: 4\n",
        "        env:\n",
        "        - name: PJRT_DEVICE\n",
        "          value: \"TPU\"\n",
        "        - name: XLA_USE_SPMD\n",
        "          value: \"1\"\n",
        "        - name: XLA_USE_BF16\n",
        "          value: \"1\"\n",
        "        - name: HF_TOKEN\n",
        "          valueFrom:\n",
        "            secretKeyRef:\n",
        "              name: hf-secret\n",
        "              key: hf_api_token\n",
        "        - name: NEW_MODEL_ID\n",
        "          value: gemma-7b-sql-kubecon-eu-2024\n",
        "        - name: LIMIT\n",
        "          value: \"10000\"\n",
        "        volumeMounts:\n",
        "        - mountPath: /dev/shm\n",
        "          name: dshm\n",
        "      volumes:\n",
        "      - name: dshm\n",
        "        emptyDir:\n",
        "          medium: Memory\n",
        "\"\"\"\n",
        "\n",
        "with open(\"finetune.yaml\", \"w\") as f:\n",
        "    f.write(K8S_JOB_YAML)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 866,
          "status": "ok",
          "timestamp": 1710698770574,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "yTmQqcRj9kz_",
        "outputId": "ff1fc799-8a02-4614-aaa1-32ebdab5eb1d"
      },
      "outputs": [],
      "source": [
        "!kubectl apply -f finetune.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYMesXi7WqCu"
      },
      "source": [
        "#### Waiting for the container to create"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKwbzKXuWvoL"
      },
      "source": [
        "Use the command below to check on the status of the container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 587,
          "status": "ok",
          "timestamp": 1710699723828,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "PXbPCrWtWqbk",
        "outputId": "f0747d91-d25c-4183-c51a-a9da91c723e0"
      },
      "outputs": [],
      "source": [
        "! kubectl get po -l job-name=tpu-job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzINwFr_WVAB"
      },
      "source": [
        "### View the logs from the running Job\n",
        "\n",
        "This will download the needed artifacts and run the finetuning code, this process will take close to 30 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAkXSoy9Ufuo"
      },
      "outputs": [],
      "source": [
        "! kubectl logs -f job/tpu-job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VosC4Fbb01S9"
      },
      "source": [
        "## Find the model on Huggingface\n",
        "\n",
        "If the Job ran successfully you can now go find the model on your Huggingface profile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af21a3cff1e0"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1528,
          "status": "ok",
          "timestamp": 1710700348433,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "911406c1561e",
        "outputId": "8eb81d2a-7808-4c36-cde5-4511759e23fb"
      },
      "outputs": [],
      "source": [
        "! kubectl delete job tpu-job\n",
        "! kubectl delete svc headless-svc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mH9DlYk3IqA5"
      },
      "outputs": [],
      "source": [
        "! kubectl delete secrets hf-secret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2acSuqPeNjJJ"
      },
      "outputs": [],
      "source": [
        "! gcloud container clusters delete {CLUSTER_NAME} \\\n",
        "  --region={LOCATION} \\\n",
        "  --quiet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_gemma_deployment_on_gke.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
